
## ğŸ“ `README.md` â€” Backend (Track 2: Patient Voice Clarity Bot)

group members
-Navou MOMO LEBON
-FUL MARVEL TIOBOU
-NGONGUE MODI ALLAN DUVAL
-NERYNNE ANAELLE DINA

---

# ğŸ§  Track 2: LLM-Enhanced Patient Education and Support â€“ Backend

This backend powers a multilingual, empathetic chatbot that helps patients better understand their diagnoses, medications, and treatments using a fine-tuned Large Language Model (LLM). Built with **FastAPI**, **LangChain**, and **TinyLLaMA**, it offers scalable support for medical question answering and retrieval-augmented generation (RAG).

---

## ğŸ“Œ Project Overview

> Due to high patient loads and limited clinician time, patients often leave medical consultations with unclear instructions or concerns. Our backend addresses this by offering a conversational AI assistant that delivers:

* Medically accurate explanations
* Culturally appropriate and compassionate responses
* Multilingual and voice-ready interaction support (handled on frontend)

---

## ğŸ§° Technologies Used

| Tool                        | Purpose                                                |
| --------------------------- | ------------------------------------------------------ |
| ğŸ **Python**               | Main programming language                              |
| âš¡ **FastAPI**               | Web framework to create RESTful API endpoints          |
| ğŸ¤— **Transformers**         | For loading and fine-tuning TinyLLaMA                  |
| ğŸ§  **LangChain**            | To support optional RAG logic with vector search       |
| ğŸ—ƒï¸ **FAISS**               | Vector similarity search for retrieved medical texts   |
| ğŸ”— **HuggingFace Pipeline** | For wrapping the LLM model inference                   |
| ğŸ“¦ **PEFT (LoRA)**          | For efficient fine-tuning with adapter layers          |
| ğŸ§ª **Uvicorn**              | ASGI server for running the FastAPI app                |
| ğŸ§  **TinyLLaMA 1.1B**       | Lightweight open-source LLM fine-tuned on medical data |

---

## ğŸ“‚ Folder Structure

```bash
Backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py            # FastAPI API endpoints
â”‚   â”œâ”€â”€ model.py           # Loads fine-tuned TinyLLaMA model
â”‚   â”œâ”€â”€ rag_chain.py       # LangChain RAG setup with FAISS
â”œâ”€â”€ data/
â”‚   â””â”€â”€ medical_data.json  # Dataset used to fine-tune model
â”œâ”€â”€ models/
â”‚   â””â”€â”€ finetuned_tinyllama/  # Saved fine-tuned LLM
â”œâ”€â”€ fintune/           # containing program to cleam,format the dataset and that to fintune our model
â”œâ”€â”€ requirement.txt       # Python dependencies
â”œâ”€â”€ .gitignore             # Ignores venv and model files

```

---

## ğŸ”§ Core Functionalities

### ğŸ—£ï¸ 1. Conversational Chat Endpoint

* **Endpoint**: `POST /chat`
* **Payload**:

  ```json
  {
    "message": "What is hypertension and how do I manage it?"
  }
  ```
* **Response**:

  ```json
  {
    "response": "Hypertension, also known as high blood pressure, is a condition..."
  }
  ```

### ğŸ§  2. Fine-tuned LLM Integration

* Uses `TinyLLaMA-1.1B-Chat-v1.0`, fine-tuned with domain-specific Q\&A pairs
* Applied **LoRA (Low-Rank Adaptation)** for efficient training
* Model loaded via `transformers` and wrapped in `HuggingFacePipeline`

### ğŸ” 3. Optional RAG Support (RetrievalQA)

* Uses **LangChain + FAISS + HuggingFace Embeddings**
* Retrieves static documents or health summaries (optional)
* Enables evidence-backed question answering
* Returns both the answer and source documents if configured

---

## ğŸ§ª Model Fine-tuning (Summary)

1. Base Model: `TinyLLaMA/TinyLLaMA-1.1B-Chat-v1.0`
2. Data: `medical_data.json` containing `prompt` and `response` pairs
3. Method: LoRA fine-tuning using PEFT and `transformers.Trainer`
4. Output: Model saved to `/models/finetuned_tinyllama/`

---

## â–¶ï¸ Running the Backend

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Run the FastAPI app

```bash
uvicorn app.main:app --reload
```

* App will be available at: `http://127.0.0.1:8000`

---

## ğŸ” Recreate the Environment

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

---

## ğŸ› ï¸ Key Files Explained

| File                | Role                                                          |
| ------------------- | ------------------------------------------------------------- |
| `main.py`           | FastAPI endpoints that receive and respond to patient queries |
| `model.py`          | Loads the fine-tuned TinyLLaMA model using HuggingFace        |
| `rag_chain.py`      | (Optional) Initializes RAG pipeline using LangChain + FAISS   |
| `medical_data.json` | JSON data used for model fine-tuning                          |
| `train.py`          | Training script using `Trainer` and LoRA config               |
| `requirements.txt`  | All backend dependencies                                      |

---

## ğŸ§  Future Improvements

* Add support for real-time streaming responses
* Enable multilingual translation for patient queries/responses
* Integrate voice-to-text and text-to-speech APIs for audio interaction
* Host backend on cloud infrastructure (e.g., GCP or HuggingFace Inference)

